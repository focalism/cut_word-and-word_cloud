# cut_word-and-word_cloud
python 无词库分词，以及结果词云化
#分词方法参考：http://www.matrix67.com/blog/archives/5044

分词时主要参考三个参数：词频，凝合程度，自由运用程度

step1：去掉文本中的标点符号，以及一些特殊符号；

step2：正则表达式将文本分割成长度不超过n（n表示词组的最大长度，例如岌岌可危长度为4）的词组；

step3：统计词频，即某个词在文本中出现的次数

step4：计算凝合程度，凝合程度表示词内部凝合程度，例如电影院的凝合度为：
adhes = P(电影院)/（P（电影）*P（院）），P表示某个词出现的概率

“作为一个无知识库的抽词程序，我们并不知道“电影院”是“电影”加“院”得来的，也并不知道“的电影”是“的”加上“电影”得来的。错误的切分方法会过高地估计该片段的凝合程度。如果我们把“电影院”看作是“电”加“影院”所得，由此得到的凝合程度会更高一些。因此，为了算出一个文本片段的凝合程度，我们需要枚举它的凝合方式——这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值”

step5：计算自由运用程度。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。“信息熵”是一个非常神奇的概念，它能够反映知道一个事件的结果后平均会给你带来多大的信息量。信息熵越高，说明运用程度越高，越有可能成为一个词组

